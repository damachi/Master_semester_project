{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jUgS3TeRSs0u"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francisdamachi/anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/francisdamachi/anaconda/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed\n",
    "from keras.layers import LSTM, CuDNNLSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "import keras\n",
    "import gensim\n",
    "\n",
    "# Version with timedistributed\n",
    "\n",
    "\n",
    "def generate(model, start= 1, length_generate = 7) :\n",
    "    print(reversed_dictionary[start])\n",
    "    start_word = start\n",
    "    built_phrase = [start_word]\n",
    "\n",
    "    seed_text = np.array([start_word])\n",
    "    seed_text = pad_sequences([seed_text],maxlen=7, padding='post')\n",
    "    predictions = model.predict_classes(seed_text, verbose=0)\n",
    "\n",
    "\n",
    "    for i in range(length_generate): \n",
    "      predict = predictions[0][i]\n",
    "      built_phrase.append(predict)\n",
    "      seed_text = pad_sequences([built_phrase], maxlen=7, padding=\"post\")\n",
    "      predictions = model.predict_classes(seed_text, verbose =0)\n",
    "\n",
    "    return built_phrase\n",
    "\n",
    "def convert_to_integer(array): \n",
    "  return [int(b) for b in array]\n",
    "\n",
    "def get_emotion_timesteps(sequence,emotion_dict,emotion_size=5):\n",
    "\n",
    "  toReturn = [np.zeros(emotion_size, dtype=bool)]\n",
    "  \n",
    "  for i in range(len(sequence)):\n",
    "    \n",
    "    word = reversed_dictionary[sequence[i]]\n",
    "    emotion_vector = emotion_dict[word]\n",
    "    added_vector = toReturn[i]|emotion_vector\n",
    "    toReturn.append(added_vector)\n",
    "  #This code transforms and array of booleans into 0 and on and 1  \n",
    "  toReturns = [convert_to_integer(emotions) for emotions in toReturn[1:]]\n",
    "\n",
    "  return np.array(toReturns)\n",
    "\n",
    "  \n",
    "class KerasBatchGenerator(object):\n",
    "\n",
    "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step):\n",
    "        self.data = data\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary = vocabulary+1\n",
    "        # this will track the progress of the batches sequentially through the\n",
    "        # data set - once the data reaches the end of the data set it will reset\n",
    "        # back to zero\n",
    "        self.current_idx = 0\n",
    "        # skip_step is the number of words which will be skipped before the next\n",
    "        # batch is skimmed from the data set\n",
    "        self.skip_step = skip_step\n",
    "        self.emotion_size = 5\n",
    "\n",
    "    def generate(self):\n",
    "        X_emotion= np.zeros((self.batch_size, self.num_steps, self.emotion_size ))\n",
    "        x = np.zeros((self.batch_size, self.num_steps))\n",
    "        y = np.zeros((self.batch_size, self.num_steps, self.vocabulary))\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if self.current_idx + self.num_steps >= len(self.data):\n",
    "                    # reset the index back to the start of the data set\n",
    "                    self.current_idx = 0\n",
    "                data_tmp = self.data[self.current_idx:self.current_idx + self.num_steps]\n",
    "                x[i, :] = data_tmp\n",
    "                temp_y = self.data[self.current_idx + 1:self.current_idx + self.num_steps + 1]\n",
    "                # convert all of temp_y into a one hot representation\n",
    "                y[i, :, :] = to_categorical(temp_y, num_classes=self.vocabulary)\n",
    "                \n",
    "                # get emotion vector\n",
    "                X_emotion[i] = get_emotion_timesteps(data_tmp,emotion_dict,self.emotion_size)\n",
    "                \n",
    "                self.current_idx += self.skip_step\n",
    "            yield [x,X_emotion], y\n",
    "\n",
    "df = pd.read_csv('LIWC2015 Results (LICW.csv).csv')\n",
    "df_emotions = df[['sad','anger','anx','negemo', 'posemo']]\n",
    "emotions = df_emotions.apply(lambda d : (d!=0.0),axis=1)\n",
    "df = pd.concat([df[['B']],emotions], axis = 1)\n",
    "\n",
    "emotion_dict = {}\n",
    "for w in df.values: \n",
    "  emotion_dict[w[0]] = w[1:] \n",
    "\n",
    "emotion_dict['nan'] = np.array([False, False, False, False, False])\n",
    "\n",
    "with open('../Data/cornell movie-dialogs corpus/movie_lines.txt',encoding='utf-8', errors ='ignore') as file:\n",
    "    data = file.readlines() \n",
    "\n",
    "data_array = []\n",
    "for line in data :\n",
    "    split_string = line.split('+++$+++')\n",
    "    dict_values = {'movieID':split_string[2], 'character name':split_string[3], 'utterance': split_string[4]}\n",
    "    \n",
    "    #data_array.append(dict_values)\n",
    "    data_array.append(dict_values['utterance'][1:-1]+' eos ')\n",
    "\n",
    "def retrieve_data(data_array):\n",
    "    for utterance in data_array:\n",
    "        #apply some tokenization of each utterance\n",
    "        yield gensim.utils.simple_preprocess(utterance, min_len=1)\n",
    "    \n",
    "utterances = list(retrieve_data(data_array))\n",
    "\n",
    "import itertools\n",
    "\n",
    "d = list(itertools.chain.from_iterable(utterances))\n",
    "test_d = ' '.join(d)\n",
    "test_d = test_d.split(' ')\n",
    "merge_d = [w.lower() for w in test_d]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jUgS3TeRSs0u"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francisdamachi/anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/francisdamachi/anaconda/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words = Counter(merge_d)\n",
    "words = [word for word in test_d if(words[word] > 1)]\n",
    "\n",
    "counter = Counter(words)\n",
    "words_for_LIWC = counter.keys()\n",
    "words_for_LIWC = list(words_for_LIWC)\n",
    "\n",
    "tokens = set(words)\n",
    "\n",
    "vocab_size = len(tokens)\n",
    "word2id = dict(zip(tokens,range(1,vocab_size+1)))\n",
    "reversed_dictionary = dict(zip(word2id.values(), word2id.keys()))\n",
    "\n",
    "\n",
    "\n",
    "complete_string = ' '.join(words)\n",
    "split_string = complete_string.split('eos')\n",
    "#remove last string which is a space \n",
    "split_string = split_string[:-1]\n",
    "\n",
    "split_data = np.array(split_string)\n",
    "clean = [sentence[1:-1] for sentence in split_data[1:]]\n",
    "\n",
    "clean.insert(0, split_data[0][:-1])\n",
    "split_data = np.array(clean)\n",
    "\n",
    "print('null' in tokens)\n",
    "print('nan' in tokens)\n",
    "print('null' in emotion_dict)\n",
    "print('nan' in emotion_dict)\n",
    "print(tokens - set(emotion_dict.keys()))\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "ratio_train_test = 0.25\n",
    "ratio_test_valid = 0.05\n",
    "\n",
    "train, test_tmp = sklearn.model_selection.train_test_split(split_data,test_size = ratio_train_test)\n",
    "test, valid = sklearn.model_selection.train_test_split(test_tmp, test_size = ratio_test_valid)\n",
    "\n",
    "def data_2_id(dataset, word_to_id):\n",
    "    data = ''.join(dataset)\n",
    "    \n",
    "    data = data.split(' ')\n",
    "    \n",
    "    #take care of strings which are empty\n",
    "    for i,v in enumerate(data):\n",
    "        if(v==''):\n",
    "            data.pop(i)\n",
    "    \n",
    "    return [word_to_id[w] for w in data]\n",
    "\n",
    "def new_sequence(sentence):\n",
    "    newString = sentence +' eos '\n",
    "    return newString\n",
    "\n",
    "def final_dataset(dataset):\n",
    "    data =  [new_sequence(sentence) for sentence in dataset]\n",
    "    \n",
    "    return data\n",
    "    \n",
    "train_final = final_dataset(train)\n",
    "train_data = data_2_id(train_final, word2id)\n",
    "\n",
    "data_path = './'\n",
    "num_steps = 20\n",
    "batch_size = 20\n",
    "skip_step = num_steps+1\n",
    "hidden_size = 50\n",
    "num_epochs = 100\n",
    "input_shape = num_steps\n",
    "vocabulary = vocab_size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def perplexity(y_true, y_pred):\n",
    "    cross_entropy = keras.backend.mean(keras.backend.categorical_crossentropy(y_true, y_pred))\n",
    "    perplexity = keras.backend.exp(cross_entropy)\n",
    "    return perplexity\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=data_path + '/model-{epoch:02d}.hdf5', verbose=1)\n",
    "\n",
    "train_data_generator = KerasBatchGenerator(train_data, num_steps, batch_size, vocabulary,\n",
    "                                           skip_step)\n",
    "\n",
    "from keras.utils import plot_model\n",
    "import graphviz\n",
    "import pydot\n",
    "\n",
    "beta = 3\n",
    "#create the emotional part model\n",
    "emotion_input = Input(shape=(5,), name='et-1')\n",
    "g = Dense(100,activation ='sigmoid')(emotion_input)\n",
    "V = Dense(vocabulary+1)(g)\n",
    "V_x_beta = Lambda(lambda x: x * beta)(V)\n",
    "model = Model(inputs=emotion_input, outputs=V_x_beta)\n",
    "\n",
    "optional_input = Input(shape=(num_steps, 5))\n",
    "et = TimeDistributed(model)(optional_input)\n",
    "\n",
    "#CuDNNLSTM\n",
    "\n",
    "main_input = Input(shape=(input_shape,), dtype='int32', name='ct-1')\n",
    "embedding = Embedding(input_dim=vocabulary+1, output_dim=200, input_length= input_shape)(main_input)\n",
    "lstm_layer_1 = LSTM(200,return_sequences=True)(embedding)\n",
    "lstm_layer_2 = LSTM(200,return_sequences=True)(lstm_layer_1)\n",
    "ct = TimeDistributed(Dense(vocabulary+1, ))(lstm_layer_2)\n",
    "\n",
    "ct_1_plus_et_1 = keras.layers.Add()([ct, et])\n",
    "softmax = keras.layers.Activation('softmax')(ct_1_plus_et_1)\n",
    "\n",
    "model = keras.models.Model(inputs=[main_input, optional_input], outputs=[softmax])\n",
    "\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',\n",
    "              metrics=[perplexity, 'accuracy'])\n",
    "\n",
    "\n",
    "plot_model(model, to_file='version1.png')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.fit_generator(train_data_generator.generate(),len(train_data)//(batch_size* num_steps),epochs=num_epochs,callbacks=[checkpointer])\n",
    "\n",
    "\n",
    "\n",
    "data_path = './'\n",
    "num_steps = 20\n",
    "batch_size = 20\n",
    "skip_step = num_steps+1\n",
    "hidden_size = 50\n",
    "num_epochs = 100\n",
    "input_shape = num_steps\n",
    "vocabulary = vocab_size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def perplexity(y_true, y_pred):\n",
    "    cross_entropy = keras.backend.mean(keras.backend.categorical_crossentropy(y_true, y_pred))\n",
    "    perplexity = keras.backend.exp(cross_entropy)\n",
    "    return perplexity\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=data_path + '/model-{epoch:02d}.hdf5', verbose=1)\n",
    "\n",
    "train_data_generator = KerasBatchGenerator(train_data, num_steps, batch_size, vocabulary,\n",
    "                                           skip_step)\n",
    "\n",
    "from keras.utils import plot_model\n",
    "import graphviz\n",
    "import pydot\n",
    "\n",
    "beta = 3\n",
    "#create the emotional part model\n",
    "emotion_input = Input(shape=(5,), name='et-1')\n",
    "g = Dense(100,activation ='sigmoid')(emotion_input)\n",
    "V = Dense(vocabulary+1)(g)\n",
    "V_x_beta = Lambda(lambda x: x * beta)(V)\n",
    "model = Model(inputs=emotion_input, outputs=V_x_beta)\n",
    "\n",
    "optional_input = Input(shape=(num_steps, 5))\n",
    "et = TimeDistributed(model)(optional_input)\n",
    "\n",
    "#CuDNNLSTM\n",
    "\n",
    "main_input = Input(shape=(input_shape,), dtype='int32', name='ct-1')\n",
    "embedding = Embedding(input_dim=vocabulary+1, output_dim=200, input_length= input_shape)(main_input)\n",
    "lstm_layer_1 = LSTM(200,return_sequences=True)(embedding)\n",
    "lstm_layer_2 = LSTM(200,return_sequences=True)(lstm_layer_1)\n",
    "ct = TimeDistributed(Dense(vocabulary+1))(lstm_layer_2)\n",
    "\n",
    "ct_1_plus_et_1 = keras.layers.Add()([ct, et])\n",
    "softmax = keras.layers.Activation('softmax')(ct_1_plus_et_1)\n",
    "\n",
    "model = keras.models.Model(inputs=[main_input, optional_input], outputs=[softmax])\n",
    "\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',\n",
    "              metrics=[perplexity, 'accuracy'])\n",
    "\n",
    "\n",
    "plot_model(model, to_file='version1.png')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.fit_generator(train_data_generator.generate(),len(train_data)//(batch_size* num_steps),epochs=num_epochs,callbacks=[checkpointer])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def perplexity(y_true, y_pred):\n",
    "    cross_entropy = keras.backend.mean(keras.backend.categorical_crossentropy(y_true, y_pred))\n",
    "    perplexity = keras.backend.exp(cross_entropy)\n",
    "    return perplexity\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=data_path + '/model-{epoch:02d}.hdf5', verbose=1)\n",
    "\n",
    "train_data_generator = KerasBatchGenerator(train_data, num_steps, batch_size, vocabulary,\n",
    "                                           skip_step)\n",
    "\n",
    "from keras.utils import plot_model\n",
    "import graphviz\n",
    "import pydot\n",
    "\n",
    "beta = 3\n",
    "#create the emotional part model\n",
    "emotion_input = Input(shape=(5,), name='et-1')\n",
    "g = Dense(100,activation ='sigmoid')(emotion_input)\n",
    "V = Dense(vocabulary+1)(g)\n",
    "V_x_beta = Lambda(lambda x: x * beta)(V)\n",
    "model = Model(inputs=emotion_input, outputs=V_x_beta)\n",
    "\n",
    "optional_input = Input(shape=(num_steps, 5))\n",
    "et = TimeDistributed(model)(optional_input)\n",
    "\n",
    "#CuDNNLSTM\n",
    "\n",
    "main_input = Input(shape=(input_shape,), dtype='int32', name='ct-1')\n",
    "embedding = Embedding(input_dim=vocabulary+1, output_dim=200, input_length= input_shape)(main_input)\n",
    "lstm_layer_1 = CuDNNLSTM(200,return_sequences=True)(embedding)\n",
    "lstm_layer_2 = CuDNNLSTM(200,return_sequences=True)(lstm_layer_1)\n",
    "ct = TimeDistributed(Dense(vocabulary+1))(lstm_layer_2)\n",
    "\n",
    "ct_1_plus_et_1 = keras.layers.Add()([ct, et])\n",
    "softmax = keras.layers.Activation('softmax')(ct_1_plus_et_1)\n",
    "\n",
    "model = keras.models.Model(inputs=[main_input, optional_input], outputs=[softmax])\n",
    "\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',\n",
    "              metrics=[perplexity, 'accuracy'])\n",
    "\n",
    "\n",
    "plot_model(model, to_file='version1.png')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.fit_generator(train_data_generator.generate(),len(train_data)//(batch_size* num_steps),epochs=num_epochs,callbacks=[checkpointer])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "version1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
