{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nJ9061360HI4"
   },
   "source": [
    "# Version n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VlqIo8SDToPm"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed\n",
    "from keras.layers import LSTM, CuDNNLSTM, Input,Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lJuDI3jggLKe"
   },
   "source": [
    "##  Look at the emotion embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rpVR7HMCFtAb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('LIWC2015 Results (LICW.csv).csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p47dqVABkVUV"
   },
   "source": [
    "Generate a dict of as key the words and the values list of booleans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-fkZNziRg-Rq"
   },
   "outputs": [],
   "source": [
    "df_emotions = df[['sad','anger','anx','negemo', 'posemo']]\n",
    "emotions = df_emotions.apply(lambda d : (d!=0.0),axis=1)\n",
    "df = pd.concat([df[['B']],emotions], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V2phV2y8sWIO"
   },
   "source": [
    "Right now we try to get for each word its associated booleanvector represenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DQouBOCvohKu"
   },
   "outputs": [],
   "source": [
    "emotion_dict = {}\n",
    "for w in df.values: \n",
    "  emotion_dict[w[0]] = w[1:]\n",
    "\n",
    "emotion_dict['nan'] = np.array([False,False,False,False,False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TFCVOTmkT9KF"
   },
   "outputs": [],
   "source": [
    "def get_emotion_timesteps(sequence,emotion_dict, reversed_dictionary, emotion_size=5):\n",
    "\n",
    "  toReturn = [np.zeros(emotion_size, dtype=bool)]\n",
    "  \n",
    "  for i in range(len(sequence)):\n",
    "    \n",
    "    word = reversed_dictionary[sequence[i]]\n",
    "    emotion_vector = emotion_dict[word]\n",
    "    added_vector = toReturn[i]|emotion_vector\n",
    "    toReturn.append(added_vector)\n",
    "  #This code transforms and array of booleans into 0 and on and 1  \n",
    "  toReturns = [convert_to_integer(emotions) for emotions in toReturn[1:]]\n",
    "\n",
    "  return toReturns\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_integer(array): \n",
    "  return [int(b) for b in array]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data for cornell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "with open('../Data/cornell movie-dialogs corpus/movie_lines.txt',encoding='utf-8', errors ='ignore') as file:\n",
    "    data = file.readlines() \n",
    "\n",
    "data_array = []\n",
    "for line in data :\n",
    "    split_string = line.split('+++$+++')\n",
    "    dict_values = {'movieID':split_string[2], 'character name':split_string[3], 'utterance': split_string[4]}\n",
    "    \n",
    "    #data_array.append(dict_values)\n",
    "    data_array.append(dict_values['utterance'][1:-1]+' eos ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_data(data_array):\n",
    "    for utterance in data_array:\n",
    "        #apply some tokenization of each utterance\n",
    "        yield gensim.utils.simple_preprocess(utterance, min_len=1)\n",
    "    \n",
    "utterances = list(retrieve_data(data_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "d = list(itertools.chain.from_iterable(utterances))\n",
    "test_d = ' '.join(d)\n",
    "test_d = test_d.split(' ')\n",
    "merge_d = [w.lower() for w in test_d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words = Counter(merge_d)\n",
    "words = [word for word in test_d if(words[word] > 1)]\n",
    "\n",
    "counter = Counter(words)\n",
    "words_for_LIWC = counter.keys()\n",
    "words_for_LIWC = list(words_for_LIWC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = set(words)\n",
    "\n",
    "vocab_size = len(tokens)\n",
    "word2id = dict(zip(tokens,range(1,vocab_size+1)))\n",
    "reversed_dictionary = dict(zip(word2id.values(), word2id.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_string = ' '.join(words)\n",
    "split_string = complete_string.split('eos')\n",
    "#remove last string which is a space \n",
    "split_string = split_string[:-1]\n",
    "\n",
    "split_data = np.array(split_string)\n",
    "clean = [sentence[1:-1] for sentence in split_data[1:]]\n",
    "\n",
    "clean.insert(0, split_data[0][:-1])\n",
    "split_data = np.array(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word2id))\n",
    "print(split_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "ratio_train_test = 0.25\n",
    "ratio_test_valid = 0.05\n",
    "\n",
    "train, test_tmp = sklearn.model_selection.train_test_split(split_data,test_size = ratio_train_test)\n",
    "test, valid = sklearn.model_selection.train_test_split(test_tmp, test_size = ratio_test_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_2_id(dataset, word_to_id):\n",
    "    data = ''.join(dataset)\n",
    "    \n",
    "    data = data.split(' ')\n",
    "    \n",
    "    #take care of strings which are empty\n",
    "    for i,v in enumerate(data):\n",
    "        if(v==''):\n",
    "            data.pop(i)\n",
    "    \n",
    "    return [word_to_id[w] for w in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_sequence(sentence):\n",
    "    newString = sentence +' eos '\n",
    "    return newString\n",
    "\n",
    "def final_dataset(dataset):\n",
    "    data =  [new_sequence(sentence) for sentence in dataset]\n",
    "    \n",
    "    return data\n",
    "    \n",
    "train_final = final_dataset(train)\n",
    "train_data = data_2_id(train_final, word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEw batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasBatchGenerator(object):\n",
    "\n",
    "    def __init__(self, X,time_steps):\n",
    "        self.train_data = X\n",
    "        self.time_steps = time_steps\n",
    "        #self.aux_input = aux_input\n",
    "        #self.batch_size = batch_size\n",
    "        \n",
    "        self.current_idx = 0 \n",
    "        \n",
    "\n",
    "        \n",
    "    def get_data(self,train_data, time_steps, current_idx):\n",
    " \n",
    "        d = np.array(train_data[current_idx:current_idx +time_steps])\n",
    "\n",
    "        emotion_vectors = get_emotion_timesteps(d[:-1], emotion_dict, reversed_dictionary)\n",
    "\n",
    "        sequences = []\n",
    "\n",
    "        for i in range(2,time_steps+1) :\n",
    "            #[0-i)\n",
    "             sequences.append(d[:i])\n",
    "\n",
    "        #proceed to pad the sequences\n",
    "        seq = pad_sequences(np.array(sequences), maxlen=time_steps, padding='pre')\n",
    "\n",
    "        X = seq[:,:-1]\n",
    "        y = seq[:,-1] #get the true y\n",
    "\n",
    "        # for example the label 2 will have at the 0,1 vector at position[2] equals to 1 and the rest 0's\n",
    "        # vocab_size + 1 because the vocabulary index starts at 1 and not at 0\n",
    "        y = to_categorical(y, num_classes=vocab_size+1)\n",
    "        \n",
    "        \n",
    "\n",
    "        return (X,np.array(emotion_vectors),y)\n",
    "        \n",
    "    \n",
    "    def generate(self):\n",
    "          \n",
    "        skip_steps = self.time_steps - 1\n",
    "        while True:\n",
    "            if(self.current_idx>=len(self.train_data)):\n",
    "               self.current_idx = 0\n",
    "            \n",
    "            \n",
    "\n",
    "            data, emotion, target = self.get_data(train_data, time_steps, self.current_idx)\n",
    "            self.current_idx += skip_steps\n",
    "         \n",
    "            yield([data,emotion],target)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 20\n",
    "input_shape = time_steps-1\n",
    "beta = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "import graphviz\n",
    "import pydot\n",
    "\n",
    "main_input = Input(shape=(input_shape,), dtype='int32', name='ct-1')\n",
    "embedding = Embedding(input_dim=vocab_size+1, output_dim=200, input_length= input_shape)(main_input)\n",
    "lstm_layer_1 = LSTM(200,return_sequences=True)(embedding)\n",
    "lstm_layer_2 = LSTM(200)(lstm_layer_1)\n",
    "U = Dense(vocab_size+1)(lstm_layer_2)\n",
    "\n",
    "optional_input = Input(shape=(5,), name='et-1')\n",
    "g = Dense(100,activation ='sigmoid')(optional_input)\n",
    "V = Dense(vocab_size+1)(g)\n",
    "V_x_beta = Lambda(lambda x: x * beta)(V)\n",
    "\n",
    "ct_1_plus_et_1 = keras.layers.Add()([U,V_x_beta])\n",
    "softmax = keras.layers.Activation('softmax')(ct_1_plus_et_1)\n",
    "model = keras.models.Model(inputs=[main_input, optional_input], outputs=[softmax])\n",
    "\n",
    "\n",
    "#TODO remember to add in the perplexity score\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "plot_model(model, to_file='version2.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = KerasBatchGenerator(train_data,time_steps=time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(train_data_generator.generate(),len(train_data)//(time_steps),epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "working.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
