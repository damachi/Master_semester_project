{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jUgS3TeRSs0u"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed\n",
    "from keras.layers import LSTM, CuDNNLSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version with timedistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vcLk8_-iAX6e"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate(model, start= 1, length_generate = 7) :\n",
    "    print(reversed_dictionary[start])\n",
    "    start_word = start\n",
    "    built_phrase = [start_word]\n",
    "\n",
    "    seed_text = np.array([start_word])\n",
    "    seed_text = pad_sequences([seed_text],maxlen=7, padding='post')\n",
    "    predictions = model.predict_classes(seed_text, verbose=0)\n",
    "\n",
    "\n",
    "    for i in range(length_generate): \n",
    "      predict = predictions[0][i]\n",
    "      built_phrase.append(predict)\n",
    "      seed_text = pad_sequences([built_phrase], maxlen=7, padding=\"post\")\n",
    "      predictions = model.predict_classes(seed_text, verbose =0)\n",
    "\n",
    "    return built_phrase\n",
    "def read_words(filename):\n",
    "    with tf.gfile.GFile(filename, \"rb\") as f:\n",
    "        return f.read().decode(\"utf-8\").replace(\"\\n\", \"eos\").split()\n",
    "      \n",
    "def convert_to_integer(array): \n",
    "  return [int(b) for b in array]\n",
    "\n",
    "def get_emotion_timesteps(sequence,emotion_dict,emotion_size=5):\n",
    "\n",
    "  toReturn = [np.zeros(emotion_size, dtype=bool)]\n",
    "  \n",
    "  for i in range(len(sequence)):\n",
    "    \n",
    "    word = reversed_dictionary[sequence[i]]\n",
    "    emotion_vector = emotion_dict[word]\n",
    "    added_vector = toReturn[i]|emotion_vector\n",
    "    toReturn.append(added_vector)\n",
    "  #This code transforms and array of booleans into 0 and on and 1  \n",
    "  toReturns = [convert_to_integer(emotions) for emotions in toReturn[1:]]\n",
    "\n",
    "  return np.array(toReturns)\n",
    "\n",
    "\n",
    "def build_vocab(filename):\n",
    "    data = read_words(filename)\n",
    "\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(1,len(words)+1)))\n",
    "\n",
    "    return word_to_id\n",
    "  \n",
    "  \n",
    "def file_to_word_ids(filename, word_to_id):\n",
    "    data = read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "  \n",
    "  \n",
    "def load_data(file):\n",
    "    # get the data paths\n",
    "    train_path = os.path.join(data_path,file)\n",
    "    # build the complete vocabulary, then convert text data to list of integers\n",
    "    word_to_id = build_vocab(train_path)\n",
    "    train_data = file_to_word_ids(train_path, word_to_id)\n",
    "    \n",
    "    vocabulary = len(word_to_id)\n",
    "    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "\n",
    "    print(train_data[:5])\n",
    "    print(word_to_id)\n",
    "    print(vocabulary)\n",
    "    print(\" \".join([reversed_dictionary[x] for x in train_data[:10]]))\n",
    "    return train_data, vocabulary, reversed_dictionary\n",
    "  \n",
    "  \n",
    "class KerasBatchGenerator(object):\n",
    "\n",
    "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step):\n",
    "        self.data = data\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary = vocabulary+1\n",
    "        # this will track the progress of the batches sequentially through the\n",
    "        # data set - once the data reaches the end of the data set it will reset\n",
    "        # back to zero\n",
    "        self.current_idx = 0\n",
    "        # skip_step is the number of words which will be skipped before the next\n",
    "        # batch is skimmed from the data set\n",
    "        self.skip_step = skip_step\n",
    "        self.emotion_size = 5\n",
    "\n",
    "    def generate(self):\n",
    "        X_emotion= np.zeros((self.batch_size, self.num_steps, self.emotion_size ))\n",
    "        x = np.zeros((self.batch_size, self.num_steps))\n",
    "        y = np.zeros((self.batch_size, self.num_steps, self.vocabulary))\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if self.current_idx + self.num_steps >= len(self.data):\n",
    "                    # reset the index back to the start of the data set\n",
    "                    self.current_idx = 0\n",
    "                data_tmp = self.data[self.current_idx:self.current_idx + self.num_steps]\n",
    "                x[i, :] = data_tmp\n",
    "                temp_y = self.data[self.current_idx + 1:self.current_idx + self.num_steps + 1]\n",
    "                # convert all of temp_y into a one hot representation\n",
    "                y[i, :, :] = to_categorical(temp_y, num_classes=self.vocabulary)\n",
    "                \n",
    "                # get emotion vector\n",
    "                X_emotion[i] = get_emotion_timesteps(data_tmp,emotion_dict,self.emotion_size)\n",
    "                \n",
    "                self.current_idx += self.skip_step\n",
    "            yield [x,X_emotion], y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DsB5Bt07_54_"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('LIWC2015 Results (LICW.csv).csv')\n",
    "df_emotions = df[['sad','anger','anx','negemo', 'posemo']]\n",
    "emotions = df_emotions.apply(lambda d : (d!=0.0),axis=1)\n",
    "df = pd.concat([df[['B']],emotions], axis = 1)\n",
    "\n",
    "\n",
    "emotion_dict = {}\n",
    "for w in df.values: \n",
    "  emotion_dict[w[0]] = w[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "150_E92DZqD3",
    "outputId": "c3473d14-a87e-4354-a6a9-bb303101a575"
   },
   "outputs": [],
   "source": [
    "data_path = './'\n",
    "num_steps = 4\n",
    "batch_size = 1\n",
    "skip_step = num_steps + 1\n",
    "hidden_size = 50\n",
    "num_epochs = 100\n",
    "input_shape = num_steps\n",
    "\n",
    "\n",
    "train_data, vocabulary, reversed_dictionary = load_data('data.txt')\n",
    "train_data_generator = KerasBatchGenerator(train_data, num_steps, batch_size, vocabulary,\n",
    "                                           skip_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "id": "i_3ne4mBu_qf",
    "outputId": "7f2e8da8-72cc-427f-c29c-5e766ddf78a2"
   },
   "outputs": [],
   "source": [
    "\"\"\"from keras.utils import plot_model\n",
    "import graphviz\n",
    "import pydot\n",
    "\n",
    "beta = 4\n",
    "#create the emotional part model\n",
    "emotion_input = Input(shape=(5,), name='et-1')\n",
    "g = Dense(100,activation ='sigmoid')(emotion_input)\n",
    "V = Dense(vocabulary+1)(g)\n",
    "V_x_beta = Lambda(lambda x: x * beta)(V)\n",
    "model = Model(inputs=emotion_input, outputs=V_x_beta)\n",
    "\n",
    "optional_input = Input(shape=(num_steps, 5))\n",
    "et = TimeDistributed(model)(optional_input)\n",
    "\n",
    "\n",
    "\n",
    "main_input = Input(shape=(input_shape,), dtype='int32', name='ct-1')\n",
    "embedding = Embedding(input_dim=vocabulary+1, output_dim=50, input_length= input_shape)(main_input)\n",
    "lstm_layer_1 = LSTM(200,return_sequences=True)(embedding)\n",
    "lstm_layer_2 = LSTM(200,return_sequences=True)(lstm_layer_1)\n",
    "ct = TimeDistributed(Dense(vocabulary+1))(lstm_layer_2)\n",
    "\n",
    "ct_1_plus_et_1 = keras.layers.Add()([ct, et])\n",
    "softmax = keras.layers.Activation('softmax')(ct_1_plus_et_1)\n",
    "\n",
    "model = keras.models.Model(inputs=[main_input, optional_input], outputs=[softmax])\n",
    "\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "plot_model(model, to_file='version1.png')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.fit_generator(train_data_generator.generate(),len(train_data)//(batch_size* num_steps),epochs=num_epochs)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3X45jlXSE9FL",
    "outputId": "449a5783-5685-4777-cc7e-5c5781daa25c"
   },
   "outputs": [],
   "source": [
    "#text = generate(model,20)\n",
    "\n",
    "# = sad, anger, anx, negemo, posemo\n",
    "\n",
    "def generate(model, start= 1, length_generate = 7) :\n",
    "    print(reversed_dictionary[start])\n",
    "    start_word = start\n",
    "    built_phrase = [start_word]\n",
    "\n",
    "    seed_text = np.array([start_word])\n",
    "    seed_text = keras.preprocessing.sequence.pad_sequences([seed_text],maxlen=7, padding='post')\n",
    "    predictions = model.predict(seed_text, verbose=0)\n",
    "\n",
    "\n",
    "    for i in range(length_generate): \n",
    "      predict = predictions[0][i]\n",
    "      built_phrase.append(predict)\n",
    "      seed_text = pad_sequences([built_phrase], maxlen=7, padding=\"post\")\n",
    "      predictions = model.predict_classes(seed_text, verbose =0)\n",
    "\n",
    "\n",
    "\n",
    "word2id  = dict([(v,k) for k,v in reversed_dictionary.items()])\n",
    "\n",
    "\n",
    "\n",
    "def emotion_per_step(i,num_step,emotions):\n",
    "    \n",
    "    emotion = {}\n",
    "    emotion['sad'] = np.array([True,False,False,False,False])\n",
    "    emotion['anger'] = np.array([False,True,False,False,False])\n",
    "    emotion['anx'] = np.array([False,False,True,False,False])\n",
    "    emotion['negemo'] = np.array([False,False,False,True,False])\n",
    "    emotion['posemo'] = np.array([False,False,False,False,True])\n",
    "    \n",
    "    assert(i < num_step)\n",
    "    \n",
    "    emotion_to_return = np.zeros(5,dtype=bool)\n",
    "    \n",
    "    for emo in emotions:\n",
    "        emotion_to_return = emotion_to_return|emotion[emo] \n",
    "    \n",
    "    emotion_to_return = [int(e) for e in  emotion_to_return]\n",
    "    \n",
    "    emotion_vector = np.zeros((1,num_step,5))\n",
    "    emotion_vector[0][i] = emotion_to_return\n",
    "    \n",
    "    return emotion_vector\n",
    "    \n",
    "    \n",
    "#in order to generate a sequence the sequence will accept two inputs in \n",
    "\n",
    "start = \"jack\"\n",
    "begin = start.split(\" \")\n",
    "start_words = [word2id[w] for w in begin] \n",
    "built_phrase = start_words\n",
    "\n",
    "\n",
    "steps = 4\n",
    "\n",
    "seed_text = keras.preprocessing.sequence.pad_sequences([built_phrase],maxlen=4, padding='post')\n",
    "\n",
    "for i in range(steps):\n",
    "\n",
    "    emo = emotion_per_step(i,steps,['negemo'])\n",
    "    \n",
    "    e\n",
    "    y = model.predict([seed_text, emo], batch_size=1)[0][i]\n",
    "    \n",
    "    prediction = np.argmax(y)\n",
    "    \n",
    "    built_phrase\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_ = np.zeros(5,dtype=bool)\n",
    "\n",
    "[int(e) for e in  emotions_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "version1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
