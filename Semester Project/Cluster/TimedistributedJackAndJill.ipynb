{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jUgS3TeRSs0u"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francisdamachi/anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/francisdamachi/anaconda/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed\n",
    "from keras.layers import LSTM, CuDNNLSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version with timedistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vcLk8_-iAX6e"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate(model, start= 1, length_generate = 7) :\n",
    "    print(reversed_dictionary[start])\n",
    "    start_word = start\n",
    "    built_phrase = [start_word]\n",
    "\n",
    "    seed_text = np.array([start_word])\n",
    "    seed_text = pad_sequences([seed_text],maxlen=7, padding='post')\n",
    "    predictions = model.predict_classes(seed_text, verbose=0)\n",
    "\n",
    "\n",
    "    for i in range(length_generate): \n",
    "      predict = predictions[0][i]\n",
    "      built_phrase.append(predict)\n",
    "      seed_text = pad_sequences([built_phrase], maxlen=7, padding=\"post\")\n",
    "      predictions = model.predict_classes(seed_text, verbose =0)\n",
    "\n",
    "    return built_phrase\n",
    "def read_words(filename):\n",
    "    with tf.gfile.GFile(filename, \"rb\") as f:\n",
    "        return f.read().decode(\"utf-8\").replace(\"\\n\", \"eos\").split()\n",
    "      \n",
    "def convert_to_integer(array): \n",
    "  return [int(b) for b in array]\n",
    "\n",
    "def get_emotion_timesteps(sequence,emotion_dict,emotion_size=5):\n",
    "\n",
    "  toReturn = [np.zeros(emotion_size, dtype=bool)]\n",
    "  \n",
    "  for i in range(len(sequence)):\n",
    "    \n",
    "    word = reversed_dictionary[sequence[i]]\n",
    "    emotion_vector = emotion_dict[word]\n",
    "    added_vector = toReturn[i]|emotion_vector\n",
    "    toReturn.append(added_vector)\n",
    "  #This code transforms and array of booleans into 0 and on and 1  \n",
    "  toReturns = [convert_to_integer(emotions) for emotions in toReturn[1:]]\n",
    "\n",
    "  return np.array(toReturns)\n",
    "\n",
    "\n",
    "def build_vocab(filename):\n",
    "    data = read_words(filename)\n",
    "\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(1,len(words)+1)))\n",
    "\n",
    "    return word_to_id\n",
    "  \n",
    "  \n",
    "def file_to_word_ids(filename, word_to_id):\n",
    "    data = read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "  \n",
    "  \n",
    "def load_data(file):\n",
    "    # get the data paths\n",
    "    train_path = os.path.join(data_path,file)\n",
    "    # build the complete vocabulary, then convert text data to list of integers\n",
    "    word_to_id = build_vocab(train_path)\n",
    "    train_data = file_to_word_ids(train_path, word_to_id)\n",
    "    \n",
    "    vocabulary = len(word_to_id)\n",
    "    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "\n",
    "    print(train_data[:5])\n",
    "    print(word_to_id)\n",
    "    print(vocabulary)\n",
    "    print(\" \".join([reversed_dictionary[x] for x in train_data[:10]]))\n",
    "    return train_data, vocabulary, reversed_dictionary\n",
    "  \n",
    "  \n",
    "class KerasBatchGenerator(object):\n",
    "\n",
    "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step):\n",
    "        self.data = data\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary = vocabulary+1\n",
    "        # this will track the progress of the batches sequentially through the\n",
    "        # data set - once the data reaches the end of the data set it will reset\n",
    "        # back to zero\n",
    "        self.current_idx = 0\n",
    "        # skip_step is the number of words which will be skipped before the next\n",
    "        # batch is skimmed from the data set\n",
    "        self.skip_step = skip_step\n",
    "        self.emotion_size = 5\n",
    "\n",
    "    def generate(self):\n",
    "        X_emotion= np.zeros((self.batch_size, self.num_steps, self.emotion_size ))\n",
    "        x = np.zeros((self.batch_size, self.num_steps))\n",
    "        y = np.zeros((self.batch_size, self.num_steps, self.vocabulary))\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if self.current_idx + self.num_steps >= len(self.data):\n",
    "                    # reset the index back to the start of the data set\n",
    "                    self.current_idx = 0\n",
    "                data_tmp = self.data[self.current_idx:self.current_idx + self.num_steps]\n",
    "                x[i, :] = data_tmp\n",
    "                temp_y = self.data[self.current_idx + 1:self.current_idx + self.num_steps + 1]\n",
    "                # convert all of temp_y into a one hot representation\n",
    "                y[i, :, :] = to_categorical(temp_y, num_classes=self.vocabulary)\n",
    "                \n",
    "                # get emotion vector\n",
    "                X_emotion[i] = get_emotion_timesteps(data_tmp,emotion_dict,self.emotion_size)\n",
    "                \n",
    "                self.current_idx += self.skip_step\n",
    "            yield [x,X_emotion], y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DsB5Bt07_54_"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('LIWC2015 Results (LICW.csv).csv')\n",
    "df_emotions = df[['sad','anger','anx','negemo', 'posemo']]\n",
    "emotions = df_emotions.apply(lambda d : (d!=0.0),axis=1)\n",
    "df = pd.concat([df[['B']],emotions], axis = 1)\n",
    "\n",
    "\n",
    "emotion_dict = {}\n",
    "for w in df.values: \n",
    "  emotion_dict[w[0]] = w[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "150_E92DZqD3",
    "outputId": "c3473d14-a87e-4354-a6a9-bb303101a575"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 2, 5, 23, 21]\n",
      "{'to': 19, 'eos': 1, 'broke': 9, 'up': 21, 'tumbling': 20, 'jill': 5, 'came': 10, 'pail': 17, 'crown': 11, 'the': 18, 'hill': 14, 'after': 7, 'water': 22, 'big': 8, 'jack': 4, 'fetch': 13, 'of': 16, 'went': 23, 'fell': 12, 'and': 2, 'down': 3, 'a': 6, 'his': 15}\n",
      "23\n",
      "jack and jill went up the hill eos to fetch\n"
     ]
    }
   ],
   "source": [
    "data_path = './'\n",
    "num_steps = 4\n",
    "batch_size = 1\n",
    "skip_step = num_steps + 1\n",
    "hidden_size = 50\n",
    "num_epochs = 100\n",
    "input_shape = num_steps\n",
    "\n",
    "\n",
    "train_data, vocabulary, reversed_dictionary = load_data('data.txt')\n",
    "train_data_generator = KerasBatchGenerator(train_data, num_steps, batch_size, vocabulary,\n",
    "                                           skip_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(y_true, y_pred):\n",
    "    cross_entropy = keras.backend.mean(keras.backend.categorical_crossentropy(y_true, y_pred))\n",
    "    perplexity = keras.backend.exp(cross_entropy) # i don't exp(x * ln(2))\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "id": "i_3ne4mBu_qf",
    "outputId": "7f2e8da8-72cc-427f-c29c-5e766ddf78a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ct-1 (InputLayer)               (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 4, 50)        1200        ct-1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 4, 200)       200800      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 4, 200)       320800      lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 4, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 4, 24)        4824        lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 4, 24)        3024        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 4, 24)        0           time_distributed_4[0][0]         \n",
      "                                                                 time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 4, 24)        0           add_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 530,648\n",
      "Trainable params: 530,648\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "import graphviz\n",
    "import pydot\n",
    "\n",
    "beta = 4\n",
    "#create the emotional part model\n",
    "emotion_input = Input(shape=(5,), name='et-1')\n",
    "g = Dense(100,activation ='sigmoid')(emotion_input)\n",
    "V = Dense(vocabulary+1)(g)\n",
    "V_x_beta = Lambda(lambda x: x * beta)(V)\n",
    "model = Model(inputs=emotion_input, outputs=V_x_beta)\n",
    "\n",
    "optional_input = Input(shape=(num_steps, 5))\n",
    "et = TimeDistributed(model)(optional_input)\n",
    "\n",
    "\n",
    "\n",
    "main_input = Input(shape=(input_shape,), dtype='int32', name='ct-1')\n",
    "embedding = Embedding(input_dim=vocabulary+1, output_dim=50, input_length= input_shape)(main_input)\n",
    "lstm_layer_1 = LSTM(200,return_sequences=True)(embedding)\n",
    "lstm_layer_2 = LSTM(200,return_sequences=True)(lstm_layer_1)\n",
    "ct = TimeDistributed(Dense(vocabulary+1))(lstm_layer_2)\n",
    "\n",
    "ct_1_plus_et_1 = keras.layers.Add()([ct, et])\n",
    "softmax = keras.layers.Activation('softmax')(ct_1_plus_et_1)\n",
    "\n",
    "model = keras.models.Model(inputs=[main_input, optional_input], outputs=[softmax])\n",
    "\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',\n",
    "              metrics=[perplexity])\n",
    "\n",
    "\n",
    "plot_model(model, to_file='version1.png')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rjgk3tEcANvA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 3s 463ms/step - loss: 6.8063 - perplexity: 2038.7094\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 5.1435 - perplexity: 453.7616\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 4.0327 - perplexity: 146.1809\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 3.4344 - perplexity: 63.0276\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 3.1794 - perplexity: 37.0246\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 2.9249 - perplexity: 22.7952\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 2.8091 - perplexity: 17.5941\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 2.6017 - perplexity: 13.6480\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 2.5281 - perplexity: 12.7130\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 2.4611 - perplexity: 11.9025\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 2.3201 - perplexity: 10.2838\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 2.2359 - perplexity: 9.4583\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 2.1507 - perplexity: 8.6771\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 2.0559 - perplexity: 7.8824\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 1.9472 - perplexity: 7.0656\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 1.8183 - perplexity: 6.2016\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.6804 - perplexity: 5.3924\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.5435 - perplexity: 4.6999\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 1.3608 - perplexity: 3.9447\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 1.2729 - perplexity: 3.6067\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.1532 - perplexity: 3.2029\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 1.0717 - perplexity: 2.9496\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 1.0074 - perplexity: 2.7652\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.9557 - perplexity: 2.6136\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.8228 - perplexity: 2.2963\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.7787 - perplexity: 2.1921\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.6980 - perplexity: 2.0208\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.6462 - perplexity: 1.9179\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 0.6055 - perplexity: 1.8403\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.5501 - perplexity: 1.7393\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 0.4389 - perplexity: 1.5583\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 0.4601 - perplexity: 1.5914\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.4413 - perplexity: 1.5570\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.4782 - perplexity: 453.7119\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.3915 - perplexity: 1.4844\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.3782 - perplexity: 1.4676\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 0.3200 - perplexity: 1.3835\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.2939 - perplexity: 1.3452\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.2681 - perplexity: 1.3095\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.2400 - perplexity: 1.2725\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.1987 - perplexity: 1.2209\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 0.1918 - perplexity: 1.2121\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1574 - perplexity: 1.1713\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1481 - perplexity: 1.1600\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.1335 - perplexity: 1.1431\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.1176 - perplexity: 1.1250\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.1056 - perplexity: 1.1115\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 1.1685 - perplexity: 291.6160\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.1035 - perplexity: 1.1105\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 38ms/step - loss: 0.1211 - perplexity: 1.1348\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.1092 - perplexity: 1.1199\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 0.1073 - perplexity: 1.1161\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0722 - perplexity: 1.0755\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.0758 - perplexity: 1.0793\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0560 - perplexity: 1.0578\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.0527 - perplexity: 1.0542\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0472 - perplexity: 1.0484\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0473 - perplexity: 1.0485\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0427 - perplexity: 1.0437\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 0.0374 - perplexity: 1.0381\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.0310 - perplexity: 1.0315\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 0.0332 - perplexity: 1.0338\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 1.8064 - perplexity: 1258.8660\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 38ms/step - loss: 0.1643 - perplexity: 1.2180\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.0990 - perplexity: 1.1084\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0887 - perplexity: 1.0942\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.0591 - perplexity: 1.0616\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.7668 - perplexity: 23.0597\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1776 - perplexity: 1.2546\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.0848 - perplexity: 1.0932\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0740 - perplexity: 1.0782\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0481 - perplexity: 1.0495\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0392 - perplexity: 1.0401\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.0391 - perplexity: 1.0399\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.0349 - perplexity: 1.0356\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.0325 - perplexity: 1.0331\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0296 - perplexity: 1.0301\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0275 - perplexity: 1.0279\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.0250 - perplexity: 1.0253\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.0249 - perplexity: 1.0252\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 1.6306 - perplexity: 10975.9782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0437 - perplexity: 1.0460\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0495 - perplexity: 1.0525\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0439 - perplexity: 1.0461\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 40ms/step - loss: 0.0385 - perplexity: 1.0399\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0323 - perplexity: 1.0333\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0361 - perplexity: 1.0371\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 38ms/step - loss: 0.0266 - perplexity: 1.0270\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.0234 - perplexity: 1.0237\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0219 - perplexity: 1.0222\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 0.0190 - perplexity: 1.0192\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.0194 - perplexity: 1.0196\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.0216 - perplexity: 1.0219\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.1893 - perplexity: 486.7307\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.0494 - perplexity: 1.0539\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 0.0470 - perplexity: 1.0508\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.0363 - perplexity: 1.0382\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.0295 - perplexity: 1.0306\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0277 - perplexity: 1.0284\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.0294 - perplexity: 1.0301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb33ccd5c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_data_generator.generate(),len(train_data)//(batch_size* num_steps),epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3X45jlXSE9FL",
    "outputId": "449a5783-5685-4777-cc7e-5c5781daa25c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seed_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9b69225c6c62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mseed_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seed_text' is not defined"
     ]
    }
   ],
   "source": [
    "#text = generate(model,20)\n",
    "\n",
    "# = sad, anger, anx, negemo, posemo\n",
    "\n",
    "def generate(model, start= 1, length_generate = 7) :\n",
    "    print(reversed_dictionary[start])\n",
    "    start_word = start\n",
    "    built_phrase = [start_word]\n",
    "\n",
    "    seed_text = np.array([start_word])\n",
    "    seed_text = keras.preprocessing.sequence.pad_sequences([seed_text],maxlen=7, padding='post')\n",
    "    predictions = model.predict(seed_text, verbose=0)\n",
    "\n",
    "\n",
    "    for i in range(length_generate): \n",
    "      predict = predictions[0][i]\n",
    "      built_phrase.append(predict)\n",
    "      seed_text = pad_sequences([built_phrase], maxlen=7, padding=\"post\")\n",
    "      predictions = model.predict_classes(seed_text, verbose =0)\n",
    "\n",
    "\n",
    "\n",
    "word2id  = dict([(v,k) for k,v in reversed_dictionary.items()])\n",
    "\n",
    "\n",
    "\n",
    "def emotion_per_step(i,num_step,emotions):\n",
    "    \n",
    "    emotion = {}\n",
    "    emotion['sad'] = np.array([True,False,False,False,False])\n",
    "    emotion['anger'] = np.array([False,True,False,False,False])\n",
    "    emotion['anx'] = np.array([False,False,True,False,False])\n",
    "    emotion['negemo'] = np.array([False,False,False,True,False])\n",
    "    emotion['posemo'] = np.array([False,False,False,False,True])\n",
    "    \n",
    "    assert(i < num_step)\n",
    "    \n",
    "    emotion_to_return = np.zeros(5,dtype=bool)\n",
    "    \n",
    "    for emo in emotions:\n",
    "        emotion_to_return = emotion_to_return|emotion[emo] \n",
    "    \n",
    "    emotion_to_return = [int(e) for e in  emotion_to_return]\n",
    "    \n",
    "    emotion_vector = np.zeros((1,num_step,5))\n",
    "    emotion_vector[0][i] = emotion_to_return\n",
    "    \n",
    "    return emotion_vector\n",
    "    \n",
    "    \n",
    "#in order to generate a sequence the sequence will accept two inputs in \n",
    "\n",
    "\n",
    "seed_text = keras.preprocessing.sequence.pad_sequences([seed_text],maxlen=4, padding='post')\n",
    "\n",
    "y = model.predict([seed_text, emotion], batch_size=1)[0][3]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_ = np.zeros(5,dtype=bool)\n",
    "\n",
    "[int(e) for e in  emotions_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "version1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
